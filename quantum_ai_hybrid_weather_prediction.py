# -*- coding: utf-8 -*-
"""Quantum Ai Hybrid Weather Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K06fIOTmP-ft2FrTjG3XDwGoym_1Imur

# Colab-ready Python notebook (use in Google Colab)
# Quantum‚ÄìAI Hybrid Weather Prediction (Multi-city: Temperature + Precipitation + Humidity)
# Features: Kaggle download (optional) with kaggle.json prompt, NASA POWER fallback, Quantum-encoded features (PennyLane), parallel city processing (joblib), parallel quantum encoding, tf.data + distributed LSTM training (tf.distribute.MirroredStrategy)


# %% [markdown]
# ## 0. Notes
# - Run this notebook in Google Colab. Upload your `kaggle.json` when prompted if you want to download a Kaggle dataset. If you don't have a Kaggle dataset, the notebook will fetch data from NASA POWER API for each city.
# - Cities included: New Delhi, London, Tokyo (modify `CITIES` below if you want others).

# %%
# ===============================================
# 1) Install dependencies
# ===============================================
"""

!pip install -q pennylane pennylane-qiskit qiskit  torch dask[complete] joblib kaggle requests matplotlib scikit-learn

!pip install -U pennylane pennylane-lightning[gpu] jax jaxlib tensorflow keras

!pip install pennylane pennylane-qiskit pennylane-lightning

"""# %% [markdown]
# ===============================================
# 2) Kaggle authentication (optional)
# ===============================================
"""

from google.colab import files
import os
print("üîë If you want to use a Kaggle dataset, upload kaggle.json now. Otherwise skip this cell.")
uploaded = files.upload()
if 'kaggle.json' in uploaded:
  os.makedirs('/root/.kaggle', exist_ok=True)
  with open('/root/.kaggle/kaggle.json','wb') as f:
    f.write(uploaded['kaggle.json'])
  os.chmod('/root/.kaggle/kaggle.json', 0o600)
  print('‚úÖ kaggle.json uploaded')
else:
  print('No kaggle.json uploaded ‚Äî will use NASA POWER API fallback for city data')

"""# %% [markdown]
# ===============================================
# 3) Configuration: cities, dates, and parameters
# ===============================================
"""

CITIES = {
'New_Delhi': {'lat': 28.6139, 'lon': 77.2090},
'London': {'lat': 51.5074, 'lon': -0.1278},
'Tokyo': {'lat': 35.6762, 'lon': 139.6503}
}
START = '20230101' # YYYYMMDD
END = '20231231'
SEQ_LEN = 14
TARGET_HORIZON = 1
N_QUBITS = 3
BATCH_SIZE = 32
EPOCHS = 30
N_JOBS = 4


# Variables we will use/ predict
VARS = ['T2M','PRECTOTCORR','RH2M'] # temperature, precipitation (daily), relative humidity

"""# %% [markdown]
# ===============================================
# 4) Helper: download city data (Kaggle if dataset provided, otherwise NASA POWER API)
# ===============================================
"""

import requests
import pandas as pd
from datetime import datetime
from joblib import Parallel, delayed

# Date range
START = 20230101
END = 20231231

# Cities and their coordinates
CITIES = {
    "Delhi": {"lat": 28.6139, "lon": 77.2090},
    "Mumbai": {"lat": 19.0760, "lon": 72.8777},
    "Bangalore": {"lat": 12.9716, "lon": 77.5946},
    "Kolkata": {"lat": 22.5726, "lon": 88.3639},
}
N_JOBS = 4

def fetch_from_nasa(lat, lon, start=START, end=END):
    """Fetch NASA POWER weather data (daily temperature, precipitation, humidity)"""
    url = (
        f"https://power.larc.nasa.gov/api/temporal/daily/point?"
        f"start={start}&end={end}&latitude={lat}&longitude={lon}"
        f"&parameters=T2M,PRECTOTCORR,RH2M&community=RE&format=JSON"
    )

    try:
        r = requests.get(url)
        r.raise_for_status()
        data = r.json()
    except Exception as e:
        print(f"‚ùå Error fetching data for lat={lat}, lon={lon}: {e}")
        return pd.DataFrame()

    # Parse JSON response
    try:
        df = pd.DataFrame(data["properties"]["parameter"])
        # NASA returns each parameter as a dictionary of date:value
        temp = pd.Series(df["T2M"]) if "T2M" in df else None
        precip = pd.Series(df["PRECTOTCORR"]) if "PRECTOTCORR" in df else None
        rh = pd.Series(df["RH2M"]) if "RH2M" in df else None

        # Combine into single DataFrame
        result = pd.DataFrame({
            "date": pd.to_datetime(list(data["properties"]["parameter"]["T2M"].keys())),
            "temp": list(data["properties"]["parameter"]["T2M"].values()),
            "precip": list(data["properties"]["parameter"]["PRECTOTCORR"].values()),
            "rh": list(data["properties"]["parameter"]["RH2M"].values())
        })

        result = result.sort_values("date")
        return result

    except Exception as e:
        print(f"‚ö†Ô∏è Parsing error: {e}")
        return pd.DataFrame()


def load_city(city, coords):
    print(f"Fetching for {city} ...")
    df = fetch_from_nasa(coords["lat"], coords["lon"])
    df["City"] = city
    return df


city_dfs = Parallel(n_jobs=min(len(CITIES), N_JOBS))(
    delayed(load_city)(c, coords) for c, coords in CITIES.items()
)

# Combine all data
combined = pd.concat(city_dfs)
print("‚úÖ Combined data shape:", combined.shape)
print(combined.groupby("City").size())

combined.head()

"""# %% [markdown]
# ===============================================
# 5) Preprocessing: scaling, sequence creation per city
# ===============================================
"""

# ===============================
# STEP 5: SCALE & CREATE SEQUENCES PER CITY
# ===============================

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Define sequence and target horizon lengths
SEQ_LEN = 7             # number of past days for input
TARGET_HORIZON = 1      # days ahead to predict

scalers = {}
city_sequences = {}

for city in combined['City'].unique():
    # Extract and sort data for each city
    df_city = combined[combined['City'] == city].set_index('date').sort_index()

    # Ensure the required columns exist
    for col in ['temp', 'precip', 'rh']:
        if col not in df_city.columns:
            df_city[col] = 0.0

    # Keep consistent column order and fill missing values
    df_city = df_city[['temp', 'precip', 'rh']].astype(float).fillna(method='ffill').fillna(method='bfill')

    # Scale the data for this city
    scalers[city] = MinMaxScaler()
    scaled = pd.DataFrame(
        scalers[city].fit_transform(df_city),
        index=df_city.index,
        columns=df_city.columns
    )

    # Create input sequences (X) and targets (y)
    Xs, ys, dates = [], [], []
    vals = scaled.values
    for i in range(len(vals) - SEQ_LEN - TARGET_HORIZON + 1):
        Xs.append(vals[i:i+SEQ_LEN])
        ys.append(vals[i+SEQ_LEN+TARGET_HORIZON-1])
        dates.append(df_city.index[i+SEQ_LEN+TARGET_HORIZON-1])

    # Store for each city
    city_sequences[city] = {
        'X': np.array(Xs, dtype=np.float32),
        'y': np.array(ys, dtype=np.float32),
        'dates': dates
    }

    print(f"{city} -> samples: {city_sequences[city]['X'].shape[0]}")

# ===============================
# Combine all cities' sequences
# ===============================

all_X, all_y, all_city, all_dates = [], [], [], []

for city, seq in city_sequences.items():
    all_X.append(seq['X'])
    all_y.append(seq['y'])
    all_city += [city] * len(seq['y'])
    all_dates += seq['dates']

# Convert to arrays
all_X = np.concatenate(all_X, axis=0)
all_y = np.concatenate(all_y, axis=0)

print(f"‚úÖ Combined dataset shape: X={all_X.shape}, y={all_y.shape}")

# ===============================================
# STEP 6: Quantum Encoder (PennyLane) & Parallel Encoding
# ===============================================

import pennylane as qml
import math
import numpy as np
from joblib import Parallel, delayed

# Number of qubits (depends on how many features you want to quantum-encode)
N_QUBITS = 6        # you can tune this (‚â• number of encoded features)
N_JOBS = 4           # number of parallel CPU threads

# Quantum device
dev = qml.device('default.qubit', wires=N_QUBITS)

@qml.qnode(dev, interface='autograd')
def qnode_angle(enc):
    """
    Quantum node: encodes numerical sequence statistics into quantum states.
    """
    # enc: 1D array of floats (length >= N_QUBITS)
    for i in range(N_QUBITS):
        val = float(enc[i % len(enc)])
        qml.RX(val * math.pi, wires=i)
        qml.RY(val * math.pi / 2, wires=i)

    # Simple entanglement
    for i in range(N_QUBITS - 1):
        qml.CNOT(wires=[i, i + 1])

    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]


def seq_to_quantum_features(seq):
    """
    Convert one weather time sequence to a compact quantum-encoded vector.
    Uses mean and std per feature as classical-to-quantum embedding.
    """
    # seq shape: (SEQ_LEN, n_vars)
    stats = np.concatenate([seq.mean(axis=0), seq.std(axis=0)])  # length = 6 if 3 vars
    feats = qnode_angle(stats)
    return np.array(feats, dtype=np.float32)


# Encode per city in parallel
all_X, all_q, all_y, all_city, all_dates = [], [], [], [], []

for city, data_dict in city_sequences.items():
    Xc = data_dict['X']

    # Parallel quantum feature extraction
    q_feats = Parallel(n_jobs=min(N_JOBS, 4))(
        delayed(seq_to_quantum_features)(Xc[i]) for i in range(Xc.shape[0])
    )
    q_feats = np.stack(q_feats)

    # Append all for global dataset
    all_X.append(Xc)
    all_q.append(q_feats)
    all_y.append(data_dict['y'])
    all_city += [city] * Xc.shape[0]
    all_dates += data_dict['dates']

# Combine all into numpy arrays
all_X = np.concatenate(all_X, axis=0)
all_q = np.concatenate(all_q, axis=0)
all_y = np.concatenate(all_y, axis=0)
all_city = np.array(all_city)
all_dates = np.array(all_dates)

print("‚úÖ Final dataset shapes ‚Äî")
print("X (LSTM inputs):", all_X.shape)
print("q (Quantum encodings):", all_q.shape)
print("y (Targets):", all_y.shape)

# %% [markdown]
# ===============================================
# 7) Train/Val split stratified by city
# ===============================================

from sklearn.model_selection import train_test_split
import numpy as np

# Stratified split by city (to preserve distribution)
train_idx, val_idx = train_test_split(
    np.arange(len(all_X)),
    test_size=0.2,
    shuffle=True,
    stratify=all_city
)

# Train sets
X_train_seq = all_X[train_idx]
X_train_q = all_q[train_idx]
y_train = all_y[train_idx]
city_train = all_city[train_idx]

# Validation sets
X_val_seq = all_X[val_idx]
X_val_q = all_q[val_idx]
y_val = all_y[val_idx]
city_val = all_city[val_idx]

print("‚úÖ Train/Validation split complete")
print(f"Train set: {X_train_seq.shape}, Val set: {X_val_seq.shape}")
print(f"Quantum features ‚Äî Train: {X_train_q.shape}, Val: {X_val_q.shape}")
print(f"Targets ‚Äî Train: {y_train.shape}, Val: {y_val.shape}")

# %% [markdown]
# ===============================================
# 8) Create tf.data datasets (include quantum features as separate input)
# ===============================================

import tensorflow as tf

# Define batch size (set if not already defined earlier)
BATCH_SIZE = 32

def make_tf_dataset(X_seq, X_q, y, batch_size=BATCH_SIZE, shuffle=True):
    ds = tf.data.Dataset.from_tensor_slices(((X_seq, X_q), y))
    if shuffle:
        ds = ds.shuffle(buffer_size=2048, reshuffle_each_iteration=True)
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds

# Create train and validation TensorFlow datasets
train_ds = make_tf_dataset(X_train_seq, X_train_q, y_train)
val_ds = make_tf_dataset(X_val_seq, X_val_q, y_val, shuffle=False)

print("‚úÖ TensorFlow datasets created successfully")
print(f"Train batches: {len(train_ds)} | Validation batches: {len(val_ds)}")

# %% [markdown]
# ===============================================
# 9) Build Hybrid Model (Sequence LSTM + Quantum feature dense) with MirroredStrategy
# ===============================================

import tensorflow as tf

# Define model constants if not already defined
SEQ_LEN = 7       # Adjust this to match the value you used earlier
N_QUBITS = 6      # Based on your quantum encoding output (shape was (1432, 6))

# Enable multi-GPU training (if available)
strategy = tf.distribute.MirroredStrategy()
print('‚úÖ Replicas in sync:', strategy.num_replicas_in_sync)

with strategy.scope():
    # --- LSTM branch for sequence input ---
    seq_input = tf.keras.layers.Input(shape=(SEQ_LEN, all_X.shape[2]), name='seq_input')
    x = tf.keras.layers.LSTM(128, return_sequences=True)(seq_input)
    x = tf.keras.layers.LSTM(64)(x)
    x = tf.keras.layers.Dense(32, activation='relu')(x)

    # --- Quantum feature branch ---
    q_input = tf.keras.layers.Input(shape=(N_QUBITS,), name='q_input')
    qx = tf.keras.layers.Dense(16, activation='relu')(q_input)
    qx = tf.keras.layers.Dense(8, activation='relu')(qx)

    # --- Fusion layer ---
    concat = tf.keras.layers.Concatenate()([x, qx])
    out = tf.keras.layers.Dense(32, activation='relu')(concat)
    out = tf.keras.layers.Dense(all_y.shape[1], activation='linear')(out)

    # --- Model compilation ---
    model = tf.keras.Model(inputs=[seq_input, q_input], outputs=out)
    model.compile(optimizer='adam',
                  loss='mse',
                  metrics=[tf.keras.metrics.RootMeanSquaredError()])

model.summary()

# %% [markdown]
# ===============================================
# 10) Train
# ===============================================
history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Predict on validation dataset
y_pred_all = model.predict(val_ds)

# Dictionary to store metrics per city
eval_results = {}

# Evaluate each city separately
for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue

    y_true_city = y_val[mask]
    y_pred_city = y_pred_all[mask]

    # Inverse scaling using the city-specific scaler
    inv_true = scalers[city].inverse_transform(y_true_city)
    inv_pred = scalers[city].inverse_transform(y_pred_city)

    # Compute evaluation metrics per variable
    metrics = {}
    for i, var in enumerate(['temp', 'precip', 'rh']):
        rmse = np.sqrt(mean_squared_error(inv_true[:, i], inv_pred[:, i]))
        mae = mean_absolute_error(inv_true[:, i], inv_pred[:, i])
        metrics[f'RMSE_{var}'] = rmse
        metrics[f'MAE_{var}'] = mae

    metrics['n_samples'] = mask.sum()
    eval_results[city] = metrics

    # Visualization: Actual vs Predicted for all 3 parameters
    fig, axes = plt.subplots(3, 1, figsize=(10, 10))
    vars_ = ['Temperature (¬∞C)', 'Precipitation (mm)', 'Relative Humidity (%)']

    for i, var in enumerate(vars_):
        axes[i].plot(inv_true[:200, i], label='Actual', linewidth=2)
        axes[i].plot(inv_pred[:200, i], label='Predicted', linestyle='--')
        axes[i].set_title(f'{city} ‚Äî {var}: Actual vs Predicted (Validation Subset)')
        axes[i].legend()
        axes[i].grid(True)

    plt.tight_layout()
    plt.show()

# Print summarized per-city evaluation results
print("üìä Per-city Evaluation Results (All Parameters):\n")
for city, metrics in eval_results.items():
    print(f"üèôÔ∏è {city}:")
    print(f"   RMSE ‚Äî Temp: {metrics['RMSE_temp']:.3f}, Precip: {metrics['RMSE_precip']:.3f}, RH: {metrics['RMSE_rh']:.3f}")
    print(f"   MAE  ‚Äî Temp: {metrics['MAE_temp']:.3f}, Precip: {metrics['MAE_precip']:.3f}, RH: {metrics['MAE_rh']:.3f}")
    print(f"   Samples: {metrics['n_samples']}\n")

# %% [markdown]
# ===============================================
# 12) Cross-City RMSE & MAE Comparison Visualization
# ===============================================
import matplotlib.pyplot as plt
import numpy as np

# Prepare data for plotting
cities = list(eval_results.keys())
vars_ = ['temp', 'precip', 'rh']

# Extract RMSE and MAE values
rmse_data = np.array([[eval_results[c][f'RMSE_{v}'] for v in vars_] for c in cities])
mae_data  = np.array([[eval_results[c][f'MAE_{v}']  for v in vars_] for c in cities])

x = np.arange(len(cities))
width = 0.25

# --- RMSE plot ---
plt.figure(figsize=(10,6))
for i, var in enumerate(vars_):
    plt.bar(x + i*width, rmse_data[:, i], width, label=var.capitalize())
plt.xticks(x + width, cities, rotation=20)
plt.ylabel('RMSE')
plt.title('Cross-City RMSE Comparison (Temperature / Precipitation / RH)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()

# --- MAE plot ---
plt.figure(figsize=(10,6))
for i, var in enumerate(vars_):
    plt.bar(x + i*width, mae_data[:, i], width, label=var.capitalize())
plt.xticks(x + width, cities, rotation=20)
plt.ylabel('MAE')
plt.title('Cross-City MAE Comparison (Temperature / Precipitation / RH)')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()

print("‚úÖ Cross-city RMSE and MAE plots generated successfully!")

# %% [markdown]
# ===============================================
# 13) Summary DataFrame for Per-City Evaluation Metrics
# ===============================================
import pandas as pd

# Build a DataFrame from eval_results
rows = []
for city, metrics in eval_results.items():
    row = {'City': city}
    for k, v in metrics.items():
        row[k] = v
    rows.append(row)

eval_df = pd.DataFrame(rows)

# Sort by RMSE_temp ascending (best first)
eval_df = eval_df.sort_values(by='RMSE_temp').reset_index(drop=True)

# Display neatly
print("üìä Per-City Evaluation Summary:")
display(eval_df.style.background_gradient(cmap='YlGnBu', subset=['RMSE_temp', 'MAE_temp']))

# Optional: Save to CSV
eval_df.to_csv("city_evaluation_summary.csv", index=False)
print("‚úÖ Saved as city_evaluation_summary.csv")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error

# --- Evaluate for all three weather variables ---
y_pred_all = model.predict(val_ds)

eval_results = {}
for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue

    y_true_city = y_val[mask]
    y_pred_city = y_pred_all[mask]

    # inverse scale per-city
    inv_true = scalers[city].inverse_transform(y_true_city)
    inv_pred = scalers[city].inverse_transform(y_pred_city)

    metrics = {}
    for i, var in enumerate(['temp', 'precip', 'rh']):
        if i < inv_true.shape[1]:
            rmse = np.sqrt(mean_squared_error(inv_true[:, i], inv_pred[:, i]))
            mae = mean_absolute_error(inv_true[:, i], inv_pred[:, i])
            metrics[f'RMSE_{var}'] = rmse
            metrics[f'MAE_{var}'] = mae

    metrics['n_samples'] = mask.sum()
    eval_results[city] = metrics

    # --- Visualization for first 200 samples (Temperature only) ---
    plt.figure(figsize=(10, 4))
    plt.plot(inv_true[:200, 0], label='Actual Temp')
    plt.plot(inv_pred[:200, 0], label='Pred Temp')
    plt.title(f'{city} ‚Äî Temperature: Actual vs Predicted (Validation subset)')
    plt.xlabel("Samples")
    plt.ylabel("Temperature")
    plt.legend()
    plt.show()

# --- Summary Table ---
rows = []
for city, metrics in eval_results.items():
    row = {'City': city}
    row.update(metrics)
    rows.append(row)

eval_df = pd.DataFrame(rows)

# Sort by RMSE_temp
if 'RMSE_temp' in eval_df.columns:
    eval_df = eval_df.sort_values(by='RMSE_temp').reset_index(drop=True)

print("üìä Per-City Evaluation Summary (Temp, Precip, RH):")
display(eval_df.style.background_gradient(cmap='YlGnBu', subset=[c for c in eval_df.columns if 'RMSE' in c or 'MAE' in c]))

# Save as CSV
eval_df.to_csv("city_evaluation_summary_full.csv", index=False)
print("‚úÖ Saved as city_evaluation_summary_full.csv")

# %% [markdown]
# ===============================================
# 14) Regression Accuracy Metrics (R¬≤ Score) & Visualization
# ===============================================
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Get predictions
y_pred_all = model.predict(val_ds)

# Combine all predictions and true values (already defined earlier)
inv_true_all = []
inv_pred_all = []
for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue
    inv_true = scalers[city].inverse_transform(y_val[mask])
    inv_pred = scalers[city].inverse_transform(y_pred_all[mask])
    inv_true_all.append(inv_true)
    inv_pred_all.append(inv_pred)

inv_true_all = np.concatenate(inv_true_all, axis=0)
inv_pred_all = np.concatenate(inv_pred_all, axis=0)

# Compute global metrics
r2_temp = r2_score(inv_true_all[:, 0], inv_pred_all[:, 0])
r2_precip = r2_score(inv_true_all[:, 1], inv_pred_all[:, 1])
r2_rh = r2_score(inv_true_all[:, 2], inv_pred_all[:, 2])

rmse_temp = np.sqrt(mean_squared_error(inv_true_all[:, 0], inv_pred_all[:, 0]))
mae_temp = mean_absolute_error(inv_true_all[:, 0], inv_pred_all[:, 0])

print("üìà Overall Model Accuracy Metrics:")
print(f"R¬≤ (Temperature): {r2_temp:.3f}")
print(f"R¬≤ (Precipitation): {r2_precip:.3f}")
print(f"R¬≤ (Humidity): {r2_rh:.3f}")
print(f"RMSE (Temperature): {rmse_temp:.3f}")
print(f"MAE (Temperature): {mae_temp:.3f}")

# Scatter plot for visualization
plt.figure(figsize=(6,6))
sns.scatterplot(x=inv_true_all[:,0], y=inv_pred_all[:,0], s=25, alpha=0.6)
plt.plot([inv_true_all[:,0].min(), inv_true_all[:,0].max()],
         [inv_true_all[:,0].min(), inv_true_all[:,0].max()], 'r--')
plt.title(f'Temperature ‚Äî Actual vs Predicted (R¬≤ = {r2_temp:.3f})')
plt.xlabel('Actual Temperature')
plt.ylabel('Predicted Temperature')
plt.grid(True)
plt.show()

# %% [markdown]
# ===============================================
# 13) Classification-style Confusion Matrix for Temperature
# ===============================================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score

# Get predictions again (if not already)
y_pred_all = model.predict(val_ds)

# Inverse transform temperature only
city = city_val[0]  # use one city for consistent scaler
inv_true = scalers[city].inverse_transform(y_val)
inv_pred = scalers[city].inverse_transform(y_pred_all)

# Take temperature column (index 0)
true_temp = inv_true[:, 0]
pred_temp = inv_pred[:, 0]

# Define categories
def categorize_temp(t):
    if t < 20:
        return 0   # Cold
    elif t <= 30:
        return 1   # Moderate
    else:
        return 2   # Hot

y_true_cat = np.array([categorize_temp(t) for t in true_temp])
y_pred_cat = np.array([categorize_temp(t) for t in pred_temp])

# Compute confusion matrix and accuracy
cm = confusion_matrix(y_true_cat, y_pred_cat)
acc = accuracy_score(y_true_cat, y_pred_cat)

# Display results
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=['Cold', 'Moderate', 'Hot'])
disp.plot(cmap='Blues', values_format='d')
plt.title(f'Confusion Matrix (Temperature Categories)\nOverall Accuracy = {acc*100:.2f}%')
plt.show()

print(f"‚úÖ Classification-style accuracy (Temperature): {acc*100:.2f}%")

# %% [markdown]
# ===============================================
# 14) Adaptive Temperature Category per City
# ===============================================
import numpy as np

print("üå°Ô∏è Predicted Temperature Category per City:\n")

# Get all predicted temps (for adaptive thresholds)
all_preds_temp = []
for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue
    y_pred_city = y_pred_all[mask]
    inv_pred = scalers[city].inverse_transform(y_pred_city)
    all_preds_temp.extend(inv_pred[:, 0])

all_preds_temp = np.array(all_preds_temp)
t_min, t_max = np.min(all_preds_temp), np.max(all_preds_temp)
range_span = t_max - t_min

# Dynamic cutoffs
cold_thresh = t_min + 0.33 * range_span
hot_thresh = t_min + 0.66 * range_span

for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue

    y_pred_city = y_pred_all[mask]
    inv_pred = scalers[city].inverse_transform(y_pred_city)
    avg_temp = np.mean(inv_pred[:, 0])

    # Dynamic categorization
    if avg_temp < cold_thresh:
        category = "‚ùÑÔ∏è Cold"
    elif avg_temp <= hot_thresh:
        category = "üå§ Moderate"
    else:
        category = "üî• Hot"

    print(f"{city:15s} ‚Üí Avg Pred Temp: {avg_temp:.2f}¬∞C  ‚Üí  {category}")

print(f"\nDynamic thresholds ‚Üí Cold < {cold_thresh:.2f}¬∞C, Moderate ‚â§ {hot_thresh:.2f}¬∞C, Hot > {hot_thresh:.2f}¬∞C")

# %% [markdown]
# ===============================================
# 14) Compare Actual vs Predicted Temperature Category per City
# ===============================================
import numpy as np

print("üå°Ô∏è Temperature Category Comparison per City:\n")

# Collect all true & predicted temps for dynamic thresholding
all_true_temp, all_pred_temp = [], []

for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue
    y_true_city = y_val[mask]
    y_pred_city = y_pred_all[mask]
    inv_true = scalers[city].inverse_transform(y_true_city)
    inv_pred = scalers[city].inverse_transform(y_pred_city)
    all_true_temp.extend(inv_true[:, 0])
    all_pred_temp.extend(inv_pred[:, 0])

all_true_temp = np.array(all_true_temp)
all_pred_temp = np.array(all_pred_temp)

# Dynamic thresholds based on actual temperature distribution
t_min = min(all_true_temp.min(), all_pred_temp.min())
t_max = max(all_true_temp.max(), all_pred_temp.max())
range_span = t_max - t_min

cold_thresh = t_min + 0.33 * range_span
hot_thresh = t_min + 0.66 * range_span

def categorize_temp(temp):
    """Categorize temperature into Cold / Moderate / Hot."""
    if temp < cold_thresh:
        return "‚ùÑÔ∏è Cold"
    elif temp <= hot_thresh:
        return "üå§ Moderate"
    else:
        return "üî• Hot"

# Print citywise comparison
for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue
    y_true_city = y_val[mask]
    y_pred_city = y_pred_all[mask]
    inv_true = scalers[city].inverse_transform(y_true_city)
    inv_pred = scalers[city].inverse_transform(y_pred_city)

    avg_true_temp = np.mean(inv_true[:, 0])
    avg_pred_temp = np.mean(inv_pred[:, 0])

    cat_true = categorize_temp(avg_true_temp)
    cat_pred = categorize_temp(avg_pred_temp)

    print(f"{city:12s} ‚Üí True Avg: {avg_true_temp:5.2f}¬∞C {cat_true:10s} | Pred Avg: {avg_pred_temp:5.2f}¬∞C {cat_pred}")

print(f"\nüìä Dynamic thresholds ‚Üí Cold < {cold_thresh:.2f}¬∞C, Moderate ‚â§ {hot_thresh:.2f}¬∞C, Hot > {hot_thresh:.2f}¬∞C")

# %% [markdown]
# ===============================================
# 15) Confusion Matrix for Temperature Categories
# ===============================================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# 1Ô∏è‚É£ Collect all actual & predicted temps
y_true_all, y_pred_all_temp = [], []

for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue
    y_true_city = y_val[mask]
    y_pred_city = y_pred_all[mask]
    inv_true = scalers[city].inverse_transform(y_true_city)
    inv_pred = scalers[city].inverse_transform(y_pred_city)
    y_true_all.extend(inv_true[:, 0])  # temperature
    y_pred_all_temp.extend(inv_pred[:, 0])

y_true_all = np.array(y_true_all)
y_pred_all_temp = np.array(y_pred_all_temp)

# 2Ô∏è‚É£ Determine thresholds dynamically
t_min = min(y_true_all.min(), y_pred_all_temp.min())
t_max = max(y_true_all.max(), y_pred_all_temp.max())
range_span = t_max - t_min

cold_thresh = t_min + 0.33 * range_span
hot_thresh = t_min + 0.66 * range_span

def categorize_temp_val(temp):
    if temp < cold_thresh:
        return "Cold"
    elif temp <= hot_thresh:
        return "Moderate"
    else:
        return "Hot"

# 3Ô∏è‚É£ Convert to categorical labels
true_labels = [categorize_temp_val(t) for t in y_true_all]
pred_labels = [categorize_temp_val(t) for t in y_pred_all_temp]

# 4Ô∏è‚É£ Confusion Matrix
cm = confusion_matrix(true_labels, pred_labels, labels=["Cold", "Moderate", "Hot"])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Cold", "Moderate", "Hot"])

# 5Ô∏è‚É£ Plot Confusion Matrix
plt.figure(figsize=(6,5))
disp.plot(cmap='viridis', values_format='d', colorbar=True)
plt.title("Confusion Matrix ‚Äî Temperature Category Prediction")
plt.show()

# 6Ô∏è‚É£ Print thresholds
print(f"üìä Category thresholds: Cold < {cold_thresh:.2f}¬∞C, Moderate ‚â§ {hot_thresh:.2f}¬∞C, Hot > {hot_thresh:.2f}¬∞C")

# %% [markdown]
# ===============================================
# 15) Confusion Matrix + Classification Report for Temperature Categories
# ===============================================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# 1Ô∏è‚É£ Collect all actual & predicted temperature values
y_true_all, y_pred_all_temp = [], []

for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue
    y_true_city = y_val[mask]
    y_pred_city = y_pred_all[mask]
    inv_true = scalers[city].inverse_transform(y_true_city)
    inv_pred = scalers[city].inverse_transform(y_pred_city)
    y_true_all.extend(inv_true[:, 0])  # temperature
    y_pred_all_temp.extend(inv_pred[:, 0])

y_true_all = np.array(y_true_all)
y_pred_all_temp = np.array(y_pred_all_temp)

# 2Ô∏è‚É£ Dynamic thresholds (automatically adjusts for your dataset)
t_min = min(y_true_all.min(), y_pred_all_temp.min())
t_max = max(y_true_all.max(), y_pred_all_temp.max())
range_span = t_max - t_min

cold_thresh = t_min + 0.33 * range_span
hot_thresh = t_min + 0.66 * range_span

def categorize_temp_val(temp):
    if temp < cold_thresh:
        return "Cold"
    elif temp <= hot_thresh:
        return "Moderate"
    else:
        return "Hot"

# 3Ô∏è‚É£ Convert temperatures into category labels
true_labels = [categorize_temp_val(t) for t in y_true_all]
pred_labels = [categorize_temp_val(t) for t in y_pred_all_temp]
labels = ["Cold", "Moderate", "Hot"]

# 4Ô∏è‚É£ Confusion Matrix
cm = confusion_matrix(true_labels, pred_labels, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)

# 5Ô∏è‚É£ Plot Confusion Matrix
plt.figure(figsize=(6,5))
disp.plot(cmap='plasma', values_format='d', colorbar=True)
plt.title("üå°Ô∏è Confusion Matrix ‚Äî Temperature Category Prediction")
plt.show()

# 6Ô∏è‚É£ Classification Metrics
print("üìä Category thresholds:")
print(f"  Cold < {cold_thresh:.2f}¬∞C")
print(f"  Moderate ‚â§ {hot_thresh:.2f}¬∞C")
print(f"  Hot > {hot_thresh:.2f}¬∞C\n")

print("üìà Classification Report:")
print(classification_report(true_labels, pred_labels, target_names=labels, digits=3))

# %% [markdown]
# ===============================================
# 16) Bar Chart: Precision, Recall, F1-score per Temperature Category
# ===============================================
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_recall_fscore_support

# 1Ô∏è‚É£ Compute metrics for each label
prec, rec, f1, support = precision_recall_fscore_support(
    true_labels, pred_labels, labels=labels, zero_division=0
)

# 2Ô∏è‚É£ Create grouped bar chart
x = np.arange(len(labels))
width = 0.25

plt.figure(figsize=(8,5))
plt.bar(x - width, prec, width, label='Precision')
plt.bar(x, rec, width, label='Recall')
plt.bar(x + width, f1, width, label='F1-Score')

# 3Ô∏è‚É£ Styling
plt.xticks(x, labels, fontsize=11)
plt.ylabel('Score', fontsize=12)
plt.title('üìä Model Performance by Temperature Category', fontsize=14)
plt.ylim(0, 1.05)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Predict all three features instead of only temperature
y_pred_all = model.predict(val_ds)

eval_results = {}
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

for city in np.unique(all_city):
    mask = (city_val == city)
    if mask.sum() == 0:
        continue
    y_true_city = y_val[mask]
    y_pred_city = y_pred_all[mask]
    inv_true = scalers[city].inverse_transform(y_true_city)
    inv_pred = scalers[city].inverse_transform(y_pred_city)

    metrics = {}
    for i, feat in enumerate(['Temperature', 'Precipitation', 'Humidity']):
        rmse = np.sqrt(mean_squared_error(inv_true[:, i], inv_pred[:, i]))
        mae = mean_absolute_error(inv_true[:, i], inv_pred[:, i])
        r2 = r2_score(inv_true[:, i], inv_pred[:, i])
        metrics[feat] = {'RMSE': rmse, 'MAE': mae, 'R¬≤': r2}
    eval_results[city] = metrics

import pandas as pd
metrics_df = pd.concat({k: pd.DataFrame(v).T for k, v in eval_results.items()}, axis=0)
display(metrics_df)

# %% [markdown]
# ===============================================
# 12) Save model, scalers, and encoded dataset
# ===============================================
model.save('quantum_ai_multi_city_model.h5')
import joblib
joblib.dump(scalers, 'scalers_multi_city.joblib')
np.savez('encoded_data.npz', X=all_X, q=all_q, y=all_y, city=all_city, dates=all_dates)


print('Saved model, scalers, and encoded dataset to disk.')

import shap
import numpy as np

# Sample a subset for explainability
sample_X_seq = X_train_seq[:100]
sample_X_q = X_train_q[:100]

# Flatten X_seq for concatenation with X_q
sample_X_seq_flat = sample_X_seq.reshape(sample_X_seq.shape[0], -1)

# Concatenate the flattened sequence and quantum features for SHAP background data
combined_background_data = np.concatenate([sample_X_seq_flat, sample_X_q], axis=1)

# Get dimensions for splitting later
seq_input_dim = sample_X_seq.shape[1] * sample_X_seq.shape[2] # 7 * 3 = 21
n_qubits = sample_X_q.shape[1] # 6

# Define a wrapper function for the model's prediction
# This wrapper will split the combined input back into (seq_input, q_input)
# and then call the actual model.predict
def wrapped_predict(combined_input):
    # combined_input will be (batch_size, seq_input_dim + n_qubits)
    # Split back into (batch_size, seq_input_dim) and (batch_size, n_qubits)
    seq_part = combined_input[:, :seq_input_dim]
    q_part = combined_input[:, seq_input_dim:]

    # Reshape seq_part back to original sequence shape (batch_size, SEQ_LEN, n_vars)
    seq_part_reshaped = seq_part.reshape(combined_input.shape[0], sample_X_seq.shape[1], sample_X_seq.shape[2])

    return model.predict((seq_part_reshaped, q_part), verbose=0)

# Initialize KernelExplainer with the wrapped predict function and combined background data
explainer = shap.KernelExplainer(wrapped_predict, combined_background_data)

# Now call the explainer with the data to be explained (also combined)
combined_data_to_explain = np.concatenate([sample_X_seq_flat, sample_X_q], axis=1)
# Fix: Pass nsamples to explainer.shap_values() method
shap_values = explainer.shap_values(combined_data_to_explain, nsamples=200)

# Generate meaningful feature names for the summary plot
seq_feature_names = []
for t in range(sample_X_seq.shape[1]): # Iterate over SEQ_LEN (7 timesteps)
    for var_idx, var_name in enumerate(['temp', 'precip', 'rh']): # Iterate over variables
        seq_feature_names.append(f'{var_name}_t-{sample_X_seq.shape[1] - 1 - t}') # e.g., temp_t-6

q_feature_names = [f'q_feat_{i}' for i in range(n_qubits)]
all_feature_names = seq_feature_names + q_feature_names

# Summary plot for the first output (assuming first output corresponds to a primary target like temperature)
# Fix: Select SHAP values for the first output (index 0) across all samples and all features
shap.summary_plot(shap_values[:, :, 0], combined_data_to_explain, feature_names=all_feature_names)

